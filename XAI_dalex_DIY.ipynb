{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b17be95",
   "metadata": {},
   "source": [
    "# Test feasibility XAI for Random Forest in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e094172a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not fetch URL https://pypi.org/simple/pyspark/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pyspark/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))) - skipping\n",
      "Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host='pypi.org', port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))) - skipping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))': /simple/pyspark/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))': /simple/pyspark/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))': /simple/pyspark/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))': /simple/pyspark/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1002)'))': /simple/pyspark/\n",
      "ERROR: Could not find a version that satisfies the requirement pyspark (from versions: none)\n",
      "ERROR: No matching distribution found for pyspark\n"
     ]
    }
   ],
   "source": [
    " import sys\n",
    " !{sys.executable} -m pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6233619f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m to_timestamp\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import weekofyear\n",
    "from pyspark.sql.functions import month\n",
    "from pyspark.sql.functions import sum,avg,max\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import row_number\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import lag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "721201bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark= SparkSession \\\n",
    "       .builder \\\n",
    "       .appName(\"XAIspark\") \\\n",
    "       .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26b5175",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/code/hellbuoy/online-retail-k-means-hierarchical-clustering/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fa4b124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries for dataframe and visualization\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime as dt\n",
    "\n",
    "# import required libraries for clustering\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from scipy.cluster.hierarchy import cut_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f0d342b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m pip install pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00c376a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7923d35f",
   "metadata": {},
   "outputs": [],
   "source": [
    "retail = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"data/OnlineRetail.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31e91c8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(retail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f962593b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(541909, 8)\n"
     ]
    }
   ],
   "source": [
    "print((retail.count(), len(retail.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c71316",
   "metadata": {},
   "source": [
    "## Data cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dbb872fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|InvoiceNo|StockCode|Description|Quantity|InvoiceDate|UnitPrice|CustomerID|Country|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "|        0|        0|       1454|       0|          0|        0|    135080|      0|\n",
      "+---------+---------+-----------+--------+-----------+---------+----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculating the Missing Values % contribution in DF\n",
    " \n",
    "# Find Count of Null, None, NaN of All DataFrame Columns\n",
    "from pyspark.sql.functions import col,isnan, when, count\n",
    "retail.select([count(when(isnan(c)| col(c).isNull(), c)).alias(c) for c in retail.columns]\n",
    "   ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "06f482ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(406829, 8)\n"
     ]
    }
   ],
   "source": [
    "# Droping rows having missing values\n",
    "\n",
    "retail = retail.dropna()\n",
    "print((retail.count(), len(retail.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "74192025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|CustomerID|            Amount|\n",
      "+----------+------------------+\n",
      "|     16250|            389.44|\n",
      "|     15574| 702.2500000000002|\n",
      "|     15555| 4758.200000000002|\n",
      "|     15271|2485.8199999999997|\n",
      "|     17714|             153.0|\n",
      "+----------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Attribute : Monetary\n",
    "\n",
    "retail = retail.withColumn('Amount' , f.col('Quantity')*f.col('UnitPrice'))\n",
    "\n",
    "rfm_m = retail.groupBy('CustomerID').agg(f.sum('Amount').alias('Amount'))\n",
    "\n",
    "#rfm_m = rfm_m.reset_index()\n",
    "rfm_m.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d5a71c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+\n",
      "|CustomerID|Frequency|\n",
      "+----------+---------+\n",
      "|     16250|       24|\n",
      "|     15574|      168|\n",
      "|     15555|      925|\n",
      "|     15271|      275|\n",
      "|     17714|       10|\n",
      "+----------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Attribute : Frequency\n",
    "rfm_f = retail.groupBy('CustomerID').agg(f.count('InvoiceNo').alias('Frequency'))\n",
    "\n",
    "#rfm_f = rfm_f.reset_index()\n",
    "rfm_f.select('CustomerID', 'Frequency').show(5)\n",
    "rfm_f = rfm_f.select('CustomerID', 'Frequency')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f26102b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+\n",
      "|CustomerID|            Amount|Frequency|\n",
      "+----------+------------------+---------+\n",
      "|     16250|            389.44|       24|\n",
      "|     15574| 702.2500000000002|      168|\n",
      "|     15555| 4758.200000000002|      925|\n",
      "|     15271|2485.8199999999997|      275|\n",
      "|     17714|             153.0|       10|\n",
      "+----------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merging the two dfs\n",
    "\n",
    "rfm = rfm_m.join(rfm_f, on='CustomerID', how='inner')\n",
    "rfm.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc5d72d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n",
      "|InvoiceNo|StockCode|         Description|Quantity|        InvoiceDate|UnitPrice|CustomerID|       Country|            Amount|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n",
      "|   536365|   85123A|WHITE HANGING HEA...|       6|2010-12-01 08:26:00|     2.55|     17850|United Kingdom|15.299999999999999|\n",
      "|   536365|    71053| WHITE METAL LANTERN|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|             20.34|\n",
      "|   536365|   84406B|CREAM CUPID HEART...|       8|2010-12-01 08:26:00|     2.75|     17850|United Kingdom|              22.0|\n",
      "|   536365|   84029G|KNITTED UNION FLA...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|             20.34|\n",
      "|   536365|   84029E|RED WOOLLY HOTTIE...|       6|2010-12-01 08:26:00|     3.39|     17850|United Kingdom|             20.34|\n",
      "+---------+---------+--------------------+--------+-------------------+---------+----------+--------------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# New Attribute : Recency\n",
    "\n",
    "# Convert to datetime to proper datatype\n",
    "retail = retail.withColumn(\"InvoiceDate\", to_timestamp(f.col('InvoiceDate'), 'dd-MM-yyyy HH:mm'))\n",
    "\n",
    "retail.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "41758800",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2011, 12, 9, 12, 50)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute the maximum date to know the last transaction date\n",
    "\n",
    "max_date = retail.agg(f.max('InvoiceDate')).take(1)[0][0]\n",
    "max_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "13329c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+----+\n",
      "|        InvoiceDate|           max_date|Diff|\n",
      "+-------------------+-------------------+----+\n",
      "|2010-12-01 08:26:00|2011-12-09 12:50:00| 373|\n",
      "|2010-12-01 08:26:00|2011-12-09 12:50:00| 373|\n",
      "|2010-12-01 08:26:00|2011-12-09 12:50:00| 373|\n",
      "|2010-12-01 08:26:00|2011-12-09 12:50:00| 373|\n",
      "|2010-12-01 08:26:00|2011-12-09 12:50:00| 373|\n",
      "+-------------------+-------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute the difference between max date and transaction date\n",
    "\n",
    "retail = (retail\n",
    "          .withColumn('max_date' , f.lit(max_date))\n",
    "          .withColumn('Diff' ,  f.datediff(f.col('max_date'), f.col('InvoiceDate'))))\n",
    "\n",
    "retail.select('InvoiceDate' , 'max_date' , 'Diff').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84fc0a99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+\n",
      "|CustomerID|Recency|\n",
      "+----------+-------+\n",
      "|     16250|    261|\n",
      "|     15574|    177|\n",
      "|     15555|     12|\n",
      "|     15271|      7|\n",
      "|     17714|    320|\n",
      "+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute last transaction date to get the recency of customers\n",
    "rfm_p = (retail\n",
    "         .groupby('CustomerID')\n",
    "         .agg(f.min('Diff').alias('Recency'))\n",
    "        )\n",
    "rfm_p.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ba2ec123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+-------+\n",
      "|CustomerID|            Amount|Frequency|Recency|\n",
      "+----------+------------------+---------+-------+\n",
      "|     16250|            389.44|       24|    261|\n",
      "|     15574| 702.2500000000002|      168|    177|\n",
      "|     15555| 4758.200000000002|      925|     12|\n",
      "|     15271|2485.8199999999997|      275|      7|\n",
      "|     17714|             153.0|       10|    320|\n",
      "+----------+------------------+---------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Merge tha dataframes to get the final RFM dataframe\n",
    "\n",
    "rfm = rfm.join(rfm_p, on='CustomerID', how='inner')\n",
    "rfm = rfm.select('CustomerID', 'Amount', 'Frequency', 'Recency')\n",
    "rfm.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f965f388",
   "metadata": {},
   "source": [
    "## Handling Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c16fd9f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101.10000000000001, 5639.15]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quantiles = rfm.approxQuantile(\"Amount\", [0.05, 0.95], 0)\n",
    "quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "32fa20bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4314, 4)\n"
     ]
    }
   ],
   "source": [
    "# Removing (statistical) outliers for Amount\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "rfm = rfm.filter((f.col('Amount') >= Q1 - 1.5*IQR ) & (f.col('Amount') <= Q3 + 1.5*IQR ) ) \n",
    "\n",
    "print((rfm.count(), len(rfm.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a022e4dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4301, 4)\n",
      "(4301, 4)\n",
      "(4281, 4)\n"
     ]
    }
   ],
   "source": [
    "# Removing (statistical) outliers for Amount\n",
    "quantiles = rfm.approxQuantile(\"Amount\", [0.05, 0.95], 0)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "rfm = rfm.filter((f.col('Amount') >= Q1 - 1.5*IQR ) & (f.col('Amount') <= Q3 + 1.5*IQR ) ) \n",
    "\n",
    "print((rfm.count(), len(rfm.columns)))\n",
    "# Removing (statistical) outliers for Recency\n",
    "quantiles = rfm.approxQuantile(\"Recency\", [0.05, 0.95], 0)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "rfm = rfm.filter((f.col('Recency') >= Q1 - 1.5*IQR ) & (f.col('Recency') <= Q3 + 1.5*IQR ) ) \n",
    "print((rfm.count(), len(rfm.columns)))\n",
    "\n",
    "# Removing (statistical) outliers for Frequency\n",
    "quantiles = rfm.approxQuantile(\"Frequency\", [0.05, 0.95], 0)\n",
    "Q1 = quantiles[0]\n",
    "Q3 = quantiles[1]\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "rfm = rfm.filter((f.col('Frequency') >= Q1 - 1.5*IQR ) & (f.col('Frequency') <= Q3 + 1.5*IQR ) ) \n",
    "print((rfm.count(), len(rfm.columns)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "17fb4b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_cols = ['Amount', 'Frequency', 'Recency']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93d2e0d",
   "metadata": {},
   "source": [
    "## Vector Assembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b9985bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vector assembler \n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "assemble       = VectorAssembler(inputCols= feature_cols, outputCol='features', handleInvalid=\"keep\")\n",
    "assembled_data = assemble.transform(rfm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "39f312b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+-------+--------------------+\n",
      "|CustomerID|            Amount|Frequency|Recency|            features|\n",
      "+----------+------------------+---------+-------+--------------------+\n",
      "|     16250|            389.44|       24|    261| [389.44,24.0,261.0]|\n",
      "|     15574| 702.2500000000002|      168|    177|[702.250000000000...|\n",
      "|     15271|2485.8199999999997|      275|      7|[2485.81999999999...|\n",
      "|     17714|             153.0|       10|    320|  [153.0,10.0,320.0]|\n",
      "|     17551|            306.84|       43|    359| [306.84,43.0,359.0]|\n",
      "+----------+------------------+---------+-------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "assembled_data.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6d3f15",
   "metadata": {},
   "source": [
    "## Rescaling the attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "743c8b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "scale= StandardScaler(inputCol='features',outputCol='Scaled_features')  #save this and load \n",
    "data_scale=scale.fit(assembled_data)\n",
    "data_scale_output=data_scale.transform(assembled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07527696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+-------+--------------------+--------------------+\n",
      "|CustomerID|            Amount|Frequency|Recency|            features|     Scaled_features|\n",
      "+----------+------------------+---------+-------+--------------------+--------------------+\n",
      "|     16250|            389.44|       24|    261| [389.44,24.0,261.0]|[0.23658248201602...|\n",
      "|     15574| 702.2500000000002|      168|    177|[702.250000000000...|[0.42661269514110...|\n",
      "|     15271|2485.8199999999997|      275|      7|[2485.81999999999...|[1.51012085416256...|\n",
      "|     17714|             153.0|       10|    320|  [153.0,10.0,320.0]|[0.09294658932942...|\n",
      "|     17551|            306.84|       43|    359| [306.84,43.0,359.0]|[0.18640347365909...|\n",
      "+----------+------------------+---------+-------+--------------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_scale_output.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979d4a5a",
   "metadata": {},
   "source": [
    "## Building the Model - Unsupervised learning - KMeans model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11f0bb54",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "kmeans=KMeans(featuresCol = 'Scaled_features' , k = 8)\n",
    "knn_model = kmeans.fit(data_scale_output.select('Scaled_features'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bd0b32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = knn_model.transform(data_scale_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "54f8284f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+---------+-------+--------------------+--------------------+----------+\n",
      "|CustomerID|            Amount|Frequency|Recency|            features|     Scaled_features|prediction|\n",
      "+----------+------------------+---------+-------+--------------------+--------------------+----------+\n",
      "|     16250|            389.44|       24|    261| [389.44,24.0,261.0]|[0.23658248201602...|         1|\n",
      "|     15574| 702.2500000000002|      168|    177|[702.250000000000...|[0.42661269514110...|         2|\n",
      "|     15271|2485.8199999999997|      275|      7|[2485.81999999999...|[1.51012085416256...|         3|\n",
      "|     17714|             153.0|       10|    320|  [153.0,10.0,320.0]|[0.09294658932942...|         1|\n",
      "|     17551|            306.84|       43|    359| [306.84,43.0,359.0]|[0.18640347365909...|         1|\n",
      "+----------+------------------+---------+-------+--------------------+--------------------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "transformed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6b031847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+------------------+------------------+------------------+------------------+------------------+\n",
      "|prediction|count|            Amount|           Recency|         Frequency|          client %|           sales %|\n",
      "+----------+-----+------------------+------------------+------------------+------------------+------------------+\n",
      "|         1|  465| 303.4993763440862|313.94408602150537| 20.86021505376344|10.861948142957253|1.5394658151960157|\n",
      "|         6|   88| 4441.849431818182|15.363636363636363|502.67045454545456|2.0555944872693295|22.530772349197214|\n",
      "|         3|  318| 3378.879465408807|25.852201257861637|238.13836477987422|7.4281709880868965|17.138978976903598|\n",
      "|         5|  108| 8048.664166666671|26.787037037037038|226.03703703703704| 2.522775052557814| 40.82592686625105|\n",
      "|         4|  729|1925.5212359396435|   33.039780521262|116.90534979423869| 17.02873160476524| 9.766985866232481|\n",
      "|         7|  730| 578.8339616438357| 92.91095890410959|30.876712328767123|17.052090633029664|2.9360689546027463|\n",
      "|         2|  583| 457.8640994854206|200.23842195540308|29.346483704974272|13.618313478159308| 2.322463188076489|\n",
      "|         0| 1260| 579.4784373015866|25.738888888888887|38.592063492063495|29.432375613174493|2.9393379835404163|\n",
      "+----------+-----+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = (transformed.groupBy(\"prediction\")\n",
    "            .agg(f.countDistinct(\"CustomerID\").alias(\"count\"),\n",
    "                 f.avg(\"Amount\").alias(\"Amount\"),\n",
    "                 f.avg(\"Recency\").alias(\"Recency\"),\n",
    "                 f.avg(\"Frequency\").alias(\"Frequency\")               \n",
    "                )\n",
    "            .withColumn('client %',( f.col('count')/f.sum('count').over(Window.partitionBy()) )*100   )\n",
    "            .withColumn('sales %',( f.col('Amount')/f.sum('Amount').over(Window.partitionBy()) )*100  )  \n",
    "          )\n",
    "    \n",
    "summary.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f04706",
   "metadata": {},
   "source": [
    "## Building the Model - Supervised learning -Random forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fa54ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "final_data =(transformed\n",
    "             .withColumnRenamed(\"prediction\", \"Cluster_ID\")\n",
    "             .select('Scaled_features','Cluster_ID')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d3cc1052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting train and test data\n",
    "train, test = final_data.randomSplit([0.75,0.25], seed = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f4fb0a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "rf  = RandomForestClassifier(featuresCol = 'Scaled_features', labelCol = 'Cluster_ID', maxDepth = 10)\n",
    "rfModel = rf.fit(train)\n",
    "predictions = rfModel.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a421bd",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c77c58aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9420702754036088"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol = \"Cluster_ID\", predictionCol = \"prediction\" , metricName = \"accuracy\")\n",
    "evaluator.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34af8058",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4265000020292632"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "evaluator2 = ClusteringEvaluator( featuresCol = \"Scaled_features\", predictionCol = \"prediction\" ,metricName = \"silhouette\")\n",
    "evaluator2.evaluate(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5436101f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For n_clusters=2, the silhouette score is 0.44455610706328524\n",
      "For n_clusters=3, the silhouette score is 0.6825137557251696\n",
      "For n_clusters=4, the silhouette score is 0.6461862440589253\n",
      "For n_clusters=5, the silhouette score is 0.5642148290421279\n",
      "For n_clusters=6, the silhouette score is 0.5573833641026827\n",
      "For n_clusters=7, the silhouette score is 0.5769959381636159\n",
      "For n_clusters=8, the silhouette score is 0.4674976976048298\n"
     ]
    }
   ],
   "source": [
    "# Silhouette analysis\n",
    "range_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n",
    "\n",
    "for num_clusters in range_n_clusters:\n",
    "    # intialise kmeans\n",
    "    kmeans=KMeans(featuresCol = 'Scaled_features' , k = num_clusters)\n",
    "    knn_model = kmeans.fit(data_scale_output.select('Scaled_features'))\n",
    "    transformed = knn_model.transform(data_scale_output)\n",
    "   \n",
    "    \n",
    "    # silhouette score\n",
    "    evaluator2 = ClusteringEvaluator( featuresCol = \"Scaled_features\", predictionCol = \"prediction\" ,metricName = \"silhouette\")\n",
    "    silhouette_avg = evaluator2.evaluate(transformed)\n",
    "    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e8ab55",
   "metadata": {},
   "source": [
    "## XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1c342484",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dalex in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (1.6.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from dalex) (68.0.0)\n",
      "Requirement already satisfied: numpy>=1.20.3 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from dalex) (1.23.5)\n",
      "Requirement already satisfied: pandas>=1.2.5 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from dalex) (1.5.3)\n",
      "Requirement already satisfied: tqdm>=4.61.2 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from dalex) (4.64.1)\n",
      "Requirement already satisfied: plotly>=5.1.0 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from dalex) (5.9.0)\n",
      "Requirement already satisfied: scipy>=1.6.3 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from dalex) (1.10.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=1.2.5->dalex) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from pandas>=1.2.5->dalex) (2.8.2)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from plotly>=5.1.0->dalex) (8.0.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from tqdm>=4.61.2->dalex) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\isharau\\appdata\\local\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas>=1.2.5->dalex) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install dalex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "39bcaef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex as dx "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "afb0e59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = knn_model \n",
    "X = transformed.select(feature_cols)\n",
    "y =( transformed\n",
    "    .select('prediction')\n",
    "    .withColumnRenamed(\"prediction\", \"Cluster_ID\")\n",
    "    .select('Cluster_ID')\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f76e2c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6381e92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_numpy = np.array(X.collect())\n",
    "y_numpy = np.array(y.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "9a50d9b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4281, 3)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "0a42f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2e994695",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pandas = pd.DataFrame(X_numpy, columns= feature_cols)\n",
    "y_pandas = pd.DataFrame(y_numpy, columns= ['prediction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "24eddcdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "bfa0c753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " isinstance(y_pandas, pd.DataFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ab0e1c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparation of a new explainer is initiated\n",
      "\n",
      "  -> data              : 4281 rows 3 cols\n",
      "  -> target variable   : Parameter 'y' was a pandas.DataFrame. Converted to a numpy.ndarray.\n",
      "  -> target variable   : 4281 values\n",
      "  -> model_class       : pyspark.ml.clustering.KMeansModel (default)\n",
      "  -> label             : Not specified, model's class short name will be used. (default)\n",
      "  -> predict function  : <function yhat_default at 0x000002DE4301E9E0> will be used (default)\n",
      "  -> predict function  : Accepts only pandas.DataFrame, numpy.ndarray causes problems.\n",
      "  -> model type        : regression will be used (default)\n",
      "  -> residual function : difference between y and yhat (default)\n",
      "  -> residuals         :  'residual_function' returns an Error when executed:\n",
      "An error occurred while calling z:org.apache.spark.ml.python.MLSerDe.loads.\n",
      ": net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\r\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\r\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\r\n",
      "\tat org.apache.spark.mllib.api.python.SerDeBase.loads(PythonMLLibAPI.scala:1326)\r\n",
      "\tat org.apache.spark.ml.python.MLSerDe.loads(MLSerDe.scala)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\n",
      "  -> model_info        : package pyspark\n",
      "\n",
      "A new explainer has been created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isharau\\AppData\\Local\\anaconda3\\lib\\site-packages\\dalex\\_explainer\\object.py:138: UserWarning: \n",
      "  -> predicted values  : 'predict_function' returns an Error when executed: \n",
      "An error occurred while calling z:org.apache.spark.ml.python.MLSerDe.loads.\n",
      ": net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.dtype). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n",
      "\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n",
      "\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n",
      "\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\r\n",
      "\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\r\n",
      "\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\r\n",
      "\tat org.apache.spark.mllib.api.python.SerDeBase.loads(PythonMLLibAPI.scala:1326)\r\n",
      "\tat org.apache.spark.ml.python.MLSerDe.loads(MLSerDe.scala)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\r\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n",
      "\tat java.lang.Thread.run(Thread.java:748)\r\n",
      "\n",
      "  checks.check_predict_function_and_model_type(predict_function, model_type,\n",
      "C:\\Users\\isharau\\AppData\\Local\\anaconda3\\lib\\site-packages\\dalex\\_explainer\\object.py:138: UserWarning: \n",
      "  -> predicted values  : 'predict_function' must return numpy.ndarray (1d)\n",
      "  checks.check_predict_function_and_model_type(predict_function, model_type,\n"
     ]
    }
   ],
   "source": [
    "exp = dx.Explainer(model, X_pandas, y_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "4c338ac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\isharau\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\sql\\pandas\\conversion.py:117: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.pyspark.enabled' is set to true; however, failed by the reason below:\n",
      "  Unsupported type in conversion to Arrow: VectorUDT()\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.pyspark.fallback.enabled' is set to true.\n",
      "  warn(msg)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.ml.python.MLSerDe.loads.\n: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\r\n\tat org.apache.spark.mllib.api.python.SerDeBase.loads(PythonMLLibAPI.scala:1326)\r\n\tat org.apache.spark.ml.python.MLSerDe.loads(MLSerDe.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m test_pandas \u001b[38;5;241m=\u001b[39m test\u001b[38;5;241m.\u001b[39mtoPandas()\n\u001b[1;32m----> 2\u001b[0m \u001b[43mexp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_pandas\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\dalex\\_explainer\\object.py:187\u001b[0m, in \u001b[0;36mExplainer.predict\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Make a prediction\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mThis function uses the `predict_function` attribute.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    Model predictions for given `data`.\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    185\u001b[0m checks\u001b[38;5;241m.\u001b[39mcheck_method_data(data)\n\u001b[1;32m--> 187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\dalex\\_explainer\\yhat.py:6\u001b[0m, in \u001b[0;36myhat_default\u001b[1;34m(m, d)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21myhat_default\u001b[39m(m, d):\n\u001b[1;32m----> 6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\ml\\clustering.py:705\u001b[0m, in \u001b[0;36mKMeansModel.predict\u001b[1;34m(self, value)\u001b[0m\n\u001b[0;32m    700\u001b[0m \u001b[38;5;129m@since\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m3.0.0\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    701\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict\u001b[39m(\u001b[38;5;28mself\u001b[39m, value: Vector) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    702\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;124;03m    Predict label for the given features.\u001b[39;00m\n\u001b[0;32m    704\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 705\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_java\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py:71\u001b[0m, in \u001b[0;36mJavaWrapper._call_java\u001b[1;34m(self, name, *args)\u001b[0m\n\u001b[0;32m     68\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [_py2java(sc, arg) \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, m(\u001b[38;5;241m*\u001b[39mjava_args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\ml\\wrapper.py:71\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     68\u001b[0m sc \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_active_spark_context\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m sc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m java_args \u001b[38;5;241m=\u001b[39m [\u001b[43m_py2java\u001b[49m\u001b[43m(\u001b[49m\u001b[43msc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m args]\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _java2py(sc, m(\u001b[38;5;241m*\u001b[39mjava_args))\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\ml\\common.py:90\u001b[0m, in \u001b[0;36m_py2java\u001b[1;34m(sc, obj)\u001b[0m\n\u001b[0;32m     88\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mbytearray\u001b[39m(CPickleSerializer()\u001b[38;5;241m.\u001b[39mdumps(obj))\n\u001b[0;32m     89\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 90\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[43msc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43morg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapache\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mMLSerDe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:169\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 169\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    171\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.ml.python.MLSerDe.loads.\n: net.razorvine.pickle.PickleException: expected zero arguments for construction of ClassDict (for numpy.core.multiarray._reconstruct). This happens when an unsupported/unregistered class is being unpickled that requires construction arguments. Fix it by registering a custom IObjectConstructor for this class.\r\n\tat net.razorvine.pickle.objects.ClassDictConstructor.construct(ClassDictConstructor.java:23)\r\n\tat net.razorvine.pickle.Unpickler.load_reduce(Unpickler.java:759)\r\n\tat net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:199)\r\n\tat net.razorvine.pickle.Unpickler.load(Unpickler.java:109)\r\n\tat net.razorvine.pickle.Unpickler.loads(Unpickler.java:122)\r\n\tat org.apache.spark.mllib.api.python.SerDeBase.loads(PythonMLLibAPI.scala:1326)\r\n\tat org.apache.spark.ml.python.MLSerDe.loads(MLSerDe.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.lang.reflect.Method.invoke(Method.java:498)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.lang.Thread.run(Thread.java:748)\r\n"
     ]
    }
   ],
   "source": [
    "test_pandas = test.toPandas()\n",
    "exp.predict(test_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f153cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_bd = exp.predict_parts(obs, type='break_down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a537f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_bd.result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac62ca16",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_bd.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
